---
layout: post
title: "Why Gaussian? Part I: tossing dice"
permalink: /blog/why-gaussian-part-i/
date: 2025-04-28 12:00:00 -0400
categories: article
img_width: "65%"
highlight: true
---

<div style="flex: 1; text-align: justify;">
    <h3>See <a href="{{ '/blog/why-gaussian-part-ii/' | relative_url }}">Why Gaussian? Part II: continuous additive processes</a> for part 2!</h3>
    <h3>See <a href="{{ '/blog/why-gaussian-part-iii/' | relative_url }}">Why Gaussian? Part III: not all processes are additive</a> for part 3!</h3>
    <p>
        Ever wonder why the famous bell-shaped normal distribution (also known as a Gaussian) shows up so often in nature? There are all kinds of sources of randomness out there-so why do so many different processes somehow end up with the same general shape? The answer is a very universal <strong>Central Limit Theorem</strong> (CLT). Roughly speaking, the CLT says that under the right conditions, the average of a bunch of random things tends to follow a normal distribution.
    </p>
    <p>
        If that sounds a little abstract, don't worry. The easiest way to get an intuition for it is to start simple-with a roll of a die.
    </p>
    <h4>Rolling a single die</h4>
    <p>
        When you roll a single die, you get a number from the set $[1,2,3,4,5,6]$, each with an equal probability of $1/6$. There's nothing remotely bell-shaped about this—it's flat, symmetric, and completely uniform across six outcomes. No hint of a Gaussian curve here. But things will change as we add more dice.
    </p>
    <p>
        In this post, we'll build intuition for the Central Limit Theorem by starting with something concrete; rolling dice. We'll begin with single and multiple tosses, introduce a clever algebraic tool to count outcomes, derive a closed-form formula for the distribution, and finally show how the familiar bell curve naturally arises in the limit of many rolls.
    </p>
    <figure style="text-align: center;">
        <img src="{{ site.baseurl }}/CLT_plots/dice1.png" alt="PMF - single die toss" style="width: {{ page.img_width }}; display: block; margin: auto;"/>
        <figcaption style="font-size: smaller; color: gray;">Probability mass function of a single die toss.</figcaption>
    </figure>
    <h4>Rolling two dies</h4>
    <p>
        Now, let's roll two dice instead. This time, we add the two numbers we roll together. (Rolling one die twice is basically the same idea - the important part is independence of individual throws). What numbers can we get? Since each die shows a value between 1 and 6, their sum can range from 2 to 12.
    </p>
    <p>
        But the important thing is: not all sums are equally likely. For example, there's only one way to get a 2 (rolling 1 and 1), but there are several ways to get a 7 (like 1+6, 2+5, 3+4, etc.).
    </p>
    <p>
        Why? Because we're summing two independent variables. Each possible combination (1–6 for each die) is equally likely, but their <em>sums</em> cluster around the middle values more often.
    </p>
    <p>
        Here's a breakdown:
    </p>
    <table>
        <thead> <tr> <th>Sum</th> <th>Ways to get it</th> <th>Probability</th> </tr> </thead>
        <tbody> <tr><td>2</td><td>(1,1)</td><td>1/36</td></tr> <tr><td>3</td><td>(1,2), (2,1)</td><td>2/36</td></tr> <tr><td>4</td><td>(1,3), (2,2), (3,1)</td><td>3/36</td></tr> <tr><td>5</td><td>(1,4), (2,3), (3,2), (4,1)</td><td>4/36</td></tr> <tr><td>6</td><td>(1,5), (2,4), (3,3), (4,2), (5,1)</td><td>5/36</td></tr> <tr><td>7</td><td>(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)</td><td>6/36</td></tr> <tr><td>8</td><td>(2,6), (3,5), (4,4), (5,3), (6,2)</td><td>5/36</td></tr> <tr><td>9</td><td>(3,6), (4,5), (5,4), (6,3)</td><td>4/36</td></tr> <tr><td>10</td><td>(4,6), (5,5), (6,4)</td><td>3/36</td></tr> <tr><td>11</td><td>(5,6), (6,5)</td><td>2/36</td></tr> <tr><td>12</td><td>(6,6)</td><td>1/36</td></tr> </tbody>
    </table>
    <p> We can neatly visualize the <strong>probability mass function</strong> (PMF) for the sum of two dice like this:
    </p>
    <figure style="text-align: center;">
        <img src="{{ site.baseurl }}/CLT_plots/dice2.png" alt="PMF - sum of two dice toss" style="width: {{ page.img_width }}; display: block; margin: auto;"/>
        <figcaption style="font-size: smaller; color: gray;">Probability mass function for the sum of two dice tosses.</figcaption>
    </figure>
    <p>
        Now, one might wonder: are we doomed to manually count the number of possibilities for each outcome whenever we want to bring in more dice? Well, yes and no. Ideally, we'd like a formula that gives us, for the number of dice $n$ and a possible outcome $k$, the number of possibilities $N(k; n)$, or the probability $p(k; n) = N(k; n) / (6^n)$. To do that, we use a cute little math trick.
    </p>
    <h4>Dice tosses: formula for probability</h4>
    <p>
    This trick uses polynomials to sort out the possible outcomes of dice tosses. First, notice that multiplying two terms, each in the form $x^{\text{some power}}$, simply causes the powers to add:
    </p>
    <p style="text-align: center;">
        $$
            x^a x^b = x^{a + b}
        $$
    </p>
    <p>
        Multiplying powers of $x$ adds exponents. So $x^3 \cdot x^5 = x^8$ symbolically corresponds to rolling a 3 and a 5 and summing the result.
    </p>
    <p style="text-align: center;">
        $$
            P_1(x) \equiv x^1 + x^2 + x^3 + x^4 + x^5 + x^6
        $$
    </p>
    <p>
        In a sense, this polynomial hides all possible outcomes of a single die toss in its terms. How about tossing two dice? We simply multiply two copies of this polynomial. Here's the neat part: when we multiply these polynomials, the coefficient of each $x^k$ in the expansion tells us how many ways we can get a sum of $k$ from $n$ dice.
    </p>
    <div class="math-desktop">
        $$
            \begin{aligned}
                P_2(x) &= \left( P_1 (x) \right)^2 = \left( x^1 + x^2 + x^3 + x^4 + x^5 + x^6 \right) = \\
                &= x^2 + 2 x^3 + 3 x^4 + 4 x^5 + 5 x^6 + 6 x^7 + 5 x^8 + 4 x^9 + 3 x^{10} + 2 x^{11} + x^{12}
            \end{aligned}
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                &P_2(x) = \left( P_1 (x) \right)^2 = \\
                &= \left( x^1 + x^2 + x^3 + x^4 + x^5 + x^6 \right) = \\
                &= x^2 + 2 x^3 + 3 x^4 + 4 x^5 + 5 x^6 + \\
                &+ 6 x^7 + 5 x^8 + 4 x^9 + 3 x^{10} + 2 x^{11} + x^{12}
            \end{aligned}
        $$
    </div>
    <p>
        The numbers 1, 2, 3, ... we see here are exactly the number of ways to achieve sums of 2, 3, 4, ... ! When we expand the polynomials and group together terms with the same power of $x$, the prefactor corresponds to the number of ways that power appears in the product. This gives us the general expression:
    </p>
    <div class="math-desktop">
        $$
            N (k; n) = \text{coefficient of $x^k$ in } P_n (x), \quad P_n (x) \equiv \left( P_1 (x) \right)^n
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                &N (k; n) = \text{coefficient of $x^k$ in } P_n (x), \\
                &\quad \text{where} \; P_n (x) \equiv \left( P_1 (x) \right)^n
            \end{aligned}
        $$
    </div>
    <p>
        In other words, the frequencies $N (k; n)$ appear as the coefficients of the expansion of the function $P_n (x)$,
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = \sum_{k = n}^{6n} N (k; n) x^k
        $$
    </p>
    <p>
        In this sense, $P_n (x)$ is the <strong>generating function</strong> of our frequency coefficients of $n$ dice toss sum. To move towards a more useful formula, we now look at the expression $P_1 (x)$ again. To simplify $P_1(x)$, we factor it to a more useful form. This lets us handle large powers more elegantly. We start by taking out a common factor of x
    </p>
    <p style="text-align: center;">
        $$
            P_1 (x) = x \left( 1 + x + x^2 + x^3 + x^4 + x^5 \right)
        $$
    </p>
    <p>
        Now we use a well known fact about polynomial series to simplify the expression $S \equiv 1 + x + \cdots + x^5$ (or other general power, since the logic is not dependent on the last coefficient being specifically $x^5$). We compare $S$ and $x S$:
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                S &= 1 + x + x^2 + x^3 + x^4 + x^5 \\
                x S &= x + x^2 + x^3 + x^4 + x^5 + x^6
            \end{aligned}
        $$
    </p>
    <p>
        We see, that the two differ only by the first term in $S$ (1) and last term in $x S$ ($x^6$). Thus, their difference is
    </p>
    <div class="math-desktop">
        $$
            S - x S = (1 - x) S = 1 - x^6 \quad \implies \quad 1 + x + x^2 + x^3 + x^4 + x^5 = S = \frac{1 - x^6}{1 - x}
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                &S - x S = (1 - x) S = 1 - x^6 \\
                &\implies S = \frac{1 - x^6}{1 - x}
            \end{aligned}
        $$
    </div>
    <p>
        Going back to the generating function $P_n (x)$, we now see
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = \left( P_1 (x) \right)^n = x^n \left( \frac{1 - x^6}{1 - x} \right)^n
        $$
    </p>
    <p>
        The reason we brought the generating function to this form is that the resulting expansion is somewhat simpler than using the multinomial theorem on the sum $1 + x + \cdots + x^5$. We now need to expand all pieces of $P_n$ and put them together. We start with the denominator, which is given by a simple geometric series
    </p>
    <p style="text-align: center;">
        $$
            \frac{1}{1 - x} = \sum_{j = 0}^\infty x^j = 1 + x + x^2 + \cdots
        $$
    </p>
    <p>
        Luckily, the $n$-th power of this series can be handled by using derivatives instead of a complicated multinomial expansion. First, we notice a pattern
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{1 - x} &= \frac{1}{(1 - x)^2} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{(1 - x)^2} &= \frac{2}{(1 - x)^3} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{(1 - x)^3} &= \frac{2 \cdot 3}{(1 - x)^4} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{(1 - x)^4} &= \frac{2 \cdot 3 \cdot 4}{(1 - x)^5}
            \end{aligned}
        $$
    </p>
    <p>
        From this pattern, we can generalize (proof left as a gentle exercise for the reader):
    </p>
    <div class="math-desktop">
        $$
            \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \frac{1}{1 - x} = \frac{(n - 1)!}{(1 - x)^n} \quad \implies \quad \frac{1}{(1 - x)^n} = \frac{1}{(n - 1)!} \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \frac{1}{1 - x}
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                &\frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \frac{1}{1 - x} = \frac{(n - 1)!}{(1 - x)^n} \\
                &\implies \frac{1}{(1 - x)^n} = \frac{1}{(n - 1)!} \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \frac{1}{1 - x}
            \end{aligned}
        $$
    </div>
    <p>
        Let's unleash this on our geometric series. We know, that the first non-vanishing term will be $x^{n - 1}$
    </p>
    <div class="math-desktop">
        $$
            \begin{aligned}
                \frac{1}{(1 - x)^n} &= \frac{1}{(n - 1)!} \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \left( x^{n - 1} + x^n + x^{n + 1} + \cdots \right) \\
                &= \frac{1}{(n - 1)!} \left( (n - 1)! + \frac{n!}{1!} x + \frac{(n+1)!}{2!} x^2 + \cdots \right) = \\
                &= 1 + \frac{n!}{(n - 1)! \; 1!} x + \frac{(n + 1)!}{(n - 1)! \; 2!} x^2 + \cdots = \sum_{j = 0}^\infty \frac{(n + j - 1)!}{(n-1)! \; j!} x^j
            \end{aligned}
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                \frac{1}{(1 - x)^n} &= \frac{1}{(n - 1)!} \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \sum_{k = n-1}^\infty x^k = \\
                = \frac{1}{(n - 1)!} &\left( (n - 1)! + \frac{n!}{1!} x + \frac{(n+1)!}{2!} x^2 + \cdots \right) = \\
                = 1 + &\frac{n!}{(n - 1)! \; 1!} x + \frac{(n + 1)!}{(n - 1)! \; 2!} x^2 + \cdots = \\
                &= \sum_{j = 0}^\infty \frac{(n + j - 1)!}{(n-1)! \; j!} x^j
            \end{aligned}
        $$
    </div>
    <p>
        Notice, that the term in the last expression is equivalent to the following binomial coefficient
    </p>
    <p style="text-align: center;">
        $$
            \frac{(n + j - 1)!}{(n-1)! \; j!} = \binom{n + j - 1}{n - 1}
        $$
    </p>
    <p>
        Now we express the numerator $(1 - x^6)^n$ in terms of a sum using binomial theorem
    </p>
    <p style="text-align: center;">
        $$
            (1 - x^6)^n = \sum_{\ell = 0}^n (-1)^\ell \binom{n}{\ell} x^{6 \ell}
        $$
    </p>
    <p>
        Finally, putting everything together,
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = x^n \sum_{j = 0}^\infty \binom{n + j - 1}{n - 1} x^j \sum_{\ell = 0}^n (-1)^\ell \binom{n}{\ell} x^{6 \ell}
        $$
    </p>
    <p>
        Now that we’ve derived the full expansion, we can finally extract the coefficient of $x^k$, giving us the number of ways to get the sum $k$ with $n$ dice. For a given $n \leq k \leq 6n$, the coefficient of $x^k$ in $P_n (x)$ is given by a condition $n + j + 6 \ell = k$, from which we can fix $j$ and thus get rid of one of the sums
    </p>
    <p style="text-align: center;">
        $$
            N (k;n) = \sum_{\ell = 0}^{\ell_{\text{max}}} (-1)^\ell \binom{k - 6 \ell - 1}{n - 1} \binom{n}{\ell}
        $$
    </p>
    <p>
        The upper limit of this sum can be fixed by demanding that $k - 6 \ell - 1 \geq n - 1$, and thus $\ell \leq (k - n) / 6$. If $(k - n) / 6$ is not an integer, then the upper limit is given by the largest integer not exceeding $(k - n) / 6$, i.e. the floor of $(k - n) / 6$, $\ell_{\text{max}} = \lfloor (k - n) / 6 \rfloor$.
    </p>
    <p>
        We can now use our new formula for $N(k;n)$ to find the probability distribution for $n = 3, 4, ...$
    </p>
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice3.png" alt="Plot 1" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice4.png" alt="Plot 2" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice5.png" alt="Plot 3" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice6.png" alt="Plot 3" style="width: 100%;">
        </figure>
    </div>
    <h4>Many, many dice tosses: marching towards the Gaussian limit</h4>
    <p>
        While elegant, this formula isn’t very helpful when $n$ becomes large. Summing over so many terms quickly becomes tedious. So how do we handle <strong>thousands</strong> of dice tosses? We need to zoom out—to stop caring about exact numbers and start looking at the big picture. That’s where normalization and limiting behavior come in. First, we introduce a modified random variable, $y$, that is corrected for the mean and standard deviation in $N (k; n)$. The mean is simple; we know, that the mean value of the outcome of $n$ dice is $((1 + 6) / 2) n = (7/2) n$. As for the variance, we recall our generating function for $N (k; n)$ and how it relates to the probability distribution
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = \sum_{k = n}^{6n} N (k; n) x^k
        $$
    </p>
    <p>
        Using $p (k; n) = N (k; n) / 6^n$ to rewrite the above expression for probability instead of frequency, we find
    </p>
    <p style="text-align: center;">
        $$
            G_n (x) \equiv \frac{1}{6^n} P_n (x) = \sum_{k = n}^{6n} p (k; n) x^k = \left\langle x^k \right\rangle
        $$
    </p>
    <p>
        where we realized that the sum over probabilities corresponds to a certain, special, parametric, mean value (here $x$ is the parameter, while $k$ is a random variable we sum over). That's right, the function $G_n (x)$ corresponds to the <strong>probability-generating function</strong> of the dice toss distribution! Knowing this, we can calculate various expectation values (average quantities). For example, we already know that all probabilities should sum up to 1, but still,
    </p>
    <div class="math-desktop">
        $$
            \sum_{k = n}^{6n} p (k; n) = 1 = G_n (1) = \frac{\left( 1^1 + 1^2 + 1^3 + 1^4 + 1^5 + 1^6 \right)^n}{6^n} = \frac{6^n}{6^n} = 1
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                \sum_{k = n}^{6n} p (k; n) = 1 = G_n (1) &= \\
                = \frac{\left( 1^1 + 1^2 + 1^3 + 1^4 + 1^5 + 1^6 \right)^n}{6^n} &= \frac{6^n}{6^n} = 1
            \end{aligned}
        $$
    </div>
    <p>
        What about the mean value? We need to calculate the following sum:
    </p>
    <p style="text-align: center;">
        $$
            \left\langle k \right\rangle = \sum_{k = n}^{6n} p (k; n) \, k
        $$
    </p>
    <p>
        But our original probability-generating function contains terms $x^k$, not $k$. However, we can use the following neat derivative trick:
    </p>
    <p style="text-align: center;">
        $$
            \frac{\mathrm{d}}{\mathrm{d} x} x^k = k x^{k-1} \quad \to \quad k = \left. \frac{\mathrm{d}}{\mathrm{d} x} x^k \right|_{x = 1}
        $$
    </p>
    <p>
        We can now apply this to the probability-generating function. Since the sum is finite, we can just differentiate it term by term (note - the coefficients do not contain $x$, so the derivative passes through them like Taco Bell passes through my digestive track on Friday evening) and obtain
    </p>
    <div class="math-desktop">
        $$
            \left\langle k \right\rangle = \sum_{k = n}^{6n} p (k; n) \, \left. \frac{\mathrm{d}}{\mathrm{d} x} x^k \right|_{x = 1} = \left. \frac{\mathrm{d}}{\mathrm{d} x} \sum_{k = n}^{6n} p (k; n) \, x^k \right|_{x = 1}
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                \left\langle k \right\rangle &= \sum_{k = n}^{6n} p (k; n) \, \left. \frac{\mathrm{d}}{\mathrm{d} x} x^k \right|_{x = 1} = \\
                &= \left. \frac{\mathrm{d}}{\mathrm{d} x} \sum_{k = n}^{6n} p (k; n) \, x^k \right|_{x = 1}
            \end{aligned}
        $$
    </div>
    <p>
        At this point, we can replace the sum with the probability-generating function and get
    </p>
    <p style="text-align: center;">
        $$
            \left\langle k \right\rangle = \left. \frac{\mathrm{d}}{\mathrm{d} x} G_n (x) \right|_{x = 1} = G_n^\prime (1)
        $$
    </p>
    <p>
        Now, what is $G_n^\prime (1)$? Glad you asked, calculating mean values was never easier:
    </p>
    <div class="math-desktop">
        $$
            \begin{aligned}
                G_n^\prime (1) &= \left. \frac{\mathrm{d}}{\mathrm{d} x} \left[ \left( x + x^2 + x^3 + x^4 + x^5 + x^6 \right)^n \right] \right|_{x = 1} = \\
                &= n \left( x + x^2 + x^3 + x^4 + x^5 + x^6 \right)^{n - 1} \times \\
                &\left. \times \left( 1 + 2 x + 3 x + 4 x + 5 x + 6 x \right) \right|_{x = 1} = \\
                &= 21 \cdot 6^{n - 1} n
            \end{aligned}
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                G_n^\prime (1) &= \\
                = \frac{\mathrm{d}}{\mathrm{d} x} & \left. \left[ \left( x + x^2 + x^3 + x^4 + x^5 + x^6 \right)^n \right] \right|_{x = 1} = \\
                = n &\left( x + x^2 + x^3 + x^4 + x^5 + x^6 \right)^{n - 1} \times \\
                \times & \left. \left( 1 + 2 x + 3 x + 4 x + 5 x + 6 x \right) \right|_{x = 1} = \\
                = 21 \cdot &6^{n - 1} n
            \end{aligned}
        $$
    </div>
    <p>
        Putting back the factor $1 / 6^n$, we get
    </p>
    <p style="text-align: center;">
        $$
            \mu = \left\langle k \right\rangle = \frac{21 \cdot 6^{n - 1}}{6^n} n = \frac{21}{6} n = \frac{7}{2} n
        $$
    </p>
    <p>
        as expected. Now that we know the method works, we can calculate another mean required for variance; $\langle k^2 \rangle$. To do that, we use derivatives again. First, we express $k^2$ in terms of $x^k$ as follows
    </p>
    <div class="math-desktop">
        $$
            \left. \begin{aligned}
                \frac{\mathrm{d}}{\mathrm{d} x} x^k &= k x^{k-1} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \left( x \frac{\mathrm{d}}{\mathrm{d} x} x^k \right) &= k^2 x^{k-1}
            \end{aligned} \right\} \quad \implies \quad
            k^2 = \left. \frac{\mathrm{d}}{\mathrm{d} x} \left( x \frac{\mathrm{d}}{\mathrm{d} x} x^k \right) \right|_{x = 1}
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                \frac{\mathrm{d}}{\mathrm{d} x} x^k &= k x^{k-1} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \left( x \frac{\mathrm{d}}{\mathrm{d} x} x^k \right) &= k^2 x^{k-1} \\
                \implies k^2 &= \left. \frac{\mathrm{d}}{\mathrm{d} x} \left( x \frac{\mathrm{d}}{\mathrm{d} x} x^k \right) \right|_{x = 1}
            \end{aligned}
        $$
    </div>
    <p>
        This time, the calculation gives us variance (left as a homework exercise for your favorite algebra software)
    </p>
    <p style="text-align: center;">
        $$
            \sigma^2 = \left\langle k^2 \right\rangle - \left\langle k \right\rangle^2 = \frac{35}{12} n
        $$
    </p>
    <p>
        Now, let's define a normalized random variable
    </p>
    <p style="text-align: center;">
        $$
            y = \frac{k - \mu}{\sigma}
        $$
    </p>
    <h4>Normalized dice toss statistics</h4>
    <p>
        Our goal is to figure out the <strong>probability density function</strong> (PDF) of $y$ in the limit of large $n$. While the variable $k$ is discrete (only having integer values), in the limit of large $n$, variable $y$ becomes "more and more continuous", to the point we can speak of a probability distribution, instead of a probability mass function. In the next part of the series we will prove this result for any random process, but for now, we will use the probability-generating function we derived earlier
    </p>
    <div class="math-desktop">
        $$
            \left\langle x^k \right\rangle = \sum_{k = n}^{6n} p (k; n) x^k = G_n (x) = \left( \frac{x}{6} \frac{1 - x^6}{1 - x} \right)^n
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                \left\langle x^k \right\rangle &= \sum_{k = n}^{6n} p (k; n) x^k = \\
                &= G_n (x) = \left( \frac{x}{6} \frac{1 - x^6}{1 - x} \right)^n
            \end{aligned}
        $$
    </div>
    <p>
        We will now normalize the distribution to zero mean and unit variance. We already found that the original distribution $p (k; n)$ has mean $\mu = 7n / 2$ and variance $\sigma^2 = 35 n / 12$. Changing $k \to k^\prime = \sigma \, k + \mu$ and $p (k;n) \to \sigma \, p (k^\prime ; n)$ normalizes the distribution. For a linear transformation, it can be easily shown that the probability-generating function transforms as follows
    </p>
    <div class="math-desktop">
        $$
            G_n (x) \to \sigma \, x^{- \mu / \sigma} G_n (x^{1 / \sigma}) = x^{ - \mu / \sigma} \left( \frac{x^{1/\sigma}}{6} \frac{1 - x^{6/\sigma}}{1 - x^{1/\sigma}} \right)^n
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                G_n (x) &\to \sigma \, x^{- \mu / \sigma} G_n (x^{1 / \sigma}) = \\
                &= x^{ - \mu / \sigma} \left( \frac{x^{1/\sigma}}{6} \frac{1 - x^{6/\sigma}}{1 - x^{1/\sigma}} \right)^n
            \end{aligned}
        $$
    </div>
    <p>
        Now we want to show the limit $n \to \infty$, which becomes significantly easier if we show it for $\log G_n (x)$ rather than $G_n (x)$ itself. Logarithm of the transformed $G_n (x)$ reads
    </p>
    <div class="math-desktop">
        $$
            \log G_n (x) = - \frac{\mu}{\sigma} \log x + n \log \left( \frac{x^{1/\sigma}}{6} \frac{1 - x^{6/\sigma}}{1 - x^{1/\sigma}} \right)
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                \log G_n (x) &= \\
                = - \frac{\mu}{\sigma} \log x &+ n \log \left( \frac{x^{1/\sigma}}{6} \frac{1 - x^{6/\sigma}}{1 - x^{1/\sigma}} \right)
            \end{aligned}
        $$
    </div>
    <p>
        To obtain the limit when $n \to \infty$, we can replace $t = 1 / n$ and use Taylor series. A tricky calculation later (given as a homework to a bored reader) reveals
    </p>
    <div class="math-desktop">
        $$
            \log G_n (x) = \frac{\log^2 x}{2} - \frac{37}{700 n} \log^4 x + \frac{1333}{128,\!625 n^2} \log^6 x + \cdots
        $$
    </div>
    <div class="math-mobile">
        $$
            \begin{aligned}
                \log G_n (x) = \frac{\log^2 x}{2} &- \\
                - \frac{37}{700 n} \log^4 x &+ \frac{1333}{128,\!625 n^2} \log^6 x + \cdots
            \end{aligned}
        $$
    </div>
    <p>
        All right—so far we’ve expanded the probability-generating function and seen that it reduces to a single term as $n$ increases. But does its corresponding distribution really converge to a Gaussian? To show that it does, we need to compute $\log G_{\mathcal{N}} (x)$ for the perfect Gaussian and show it is equivalent to the first term in our expansion. This is a matter of a simple integral and completing the square
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                G_\mathcal{N} (x) &= \frac{1}{\sqrt{2 \pi}} \int \limits_{-\infty}^\infty \mathrm{d} u \, x^u \, \exp \left( - \frac{u^2}{2} \right) = \\
                &= \frac{1}{\sqrt{2 \pi}} \int \limits_{-\infty}^\infty \mathrm{d} u \, \exp \left( - \frac{u^2}{2} + u \log x \right) = \\
                \frac{1}{\sqrt{2 \pi}} &\int \limits_{-\infty}^\infty \mathrm{d} u \, \exp \left[ - \frac{1}{2} \left( u - \log x \right)^2 + \frac{\log^2 x}{2} \right] = \\
                &= \exp \left( \frac{\log^2 x}{2} \right)
            \end{aligned}
        $$
    </p>
    <p>
        which is, after applying logarithm, exactly the first term of the expansion of $G_n (x)$ for large $n$.
    </p>
    <p>
        As we can see, the limit $n \to \infty$ truly converges towards a perfect Gaussian. We can check the histograms for actual dice tosses for larger $n$ and see that the process converges really quickly (here I also added the corresponding Gaussian curve for comparison):
    </p>
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice10.png" alt="Plot 1" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice20.png" alt="Plot 2" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice30.png" alt="Plot 3" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="{{ site.baseurl }}/CLT_plots/dice40.png" alt="Plot 3" style="width: 100%;">
        </figure>
    </div>
    <div style="height: 30px;"></div>
    <h4>Conclusion: why the bell curve appears</h4>
    <p>
        We’ve seen that even something as simple as rolling dice already contains the seeds of the Gaussian. As the number of dice increases, the distribution of their normalized sum smoothly morphs into a bell curve. This is the Central Limit Theorem at work: the average of many independent, finite-variance variables converges to the normal distribution.
    </p>
    <p>
        In the <a href="{{ '/blog/why-gaussian-part-ii/' | relative_url }}">next post</a>, we’ll explore how this logic extends to <strong>continuous processes</strong>, and why the Gaussian remains such a dominant force in probability, statistics, and physics.
    </p>
    <p>
        Bonus: this whole analysis can be repeated for a $s$-sided coin. If you wonder whether the convergence to a Gaussian is faster or slower, it's neither; changing the number of sides barely changes anything about the speed of convergence. The coefficients next to $t^4$ and $t^6$ can be obtained through expansion:
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                &\frac{1}{8} - \frac{s^2 + 1}{s^2 - 1} \frac{1}{n} \\
                &\frac{1}{48} + \frac{s^2 + 1}{40 (s^2 - 1)} \frac{1}{n} - \frac{s^4 + s^2 + 1}{105 (s^2 - 1)^2} \frac{1}{n^2}
            \end{aligned}
        $$
    </p>
    <p>
    and as you can see, increasing $s$ has very little effect on the size of the terms containing $n$. Thus, increasing the number of sides on a dices has virtually no effect on the speed of the convergence towards a Gaussian.
    </p>
</div>
