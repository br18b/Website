---
layout: post
title: "Why Gaussian? Part I: tossing dice"
date: 2025-04-28 12:00:00 -0400
categories: updates
img_width: "65%"
---

<div style="flex: 1; text-align: justify;">
    <p>
        Ever wonder why the famous bell-shaped normal distribution (also known as a Gaussian) shows up so often in nature? There are all kinds of sources of randomness out there-so why do so many different processes somehow end up with the same general shape? The answer is a very universal <strong>Central Limit Theorem</strong> (CLT). Roughly speaking, the CLT says that under the right conditions, the average of a bunch of random things tends to follow a normal distribution.
    </p>
    <p>
        If that sounds a little abstract, don't worry. The easiest way to get an intuition for it is to start simple-with a basic dice roll.
    </p>
    <p>
        When you roll a single die, you get a number from the set $[1,2,3,4,5,6]$, each with an equal probability of $1/6$. There's nothing remotely bell-shaped about this; it's just flat and uniform across the six numbers. Where's the Gaussian, you might ask? Hold your horses - we're getting there.
    </p>
    <figure style="text-align: center;">
    <img src="/CLT_plots/dice1.png" alt="PMF - single die toss" style="width: {{ page.img_width }}; display: block; margin: auto;"/>
    <figcaption style="font-size: smaller; color: gray;">Probability mass function of a single die toss.</figcaption>
    </figure>
    <p>
        Now, let's roll two dice instead. This time, we add the two numbers together. (Rolling one die twice is basically the same idea - the important part is independence of individual throws). What numbers can we get? Since each die shows a value between 1 and 6, their sum can range from 2 to 12.
    </p>
    <p>
        But the important thing is: not all sums are equally likely. For example, there's only one way to get a 2 (rolling 1 and 1), but there are several ways to get a 7 (like 1+6, 2+5, 3+4, etc.).
    </p>
    <p>
        Here's a breakdown:
    </p>
    <table>
        <thead> <tr> <th>Sum</th> <th>Ways to get it</th> <th>Probability</th> </tr> </thead>
        <tbody> <tr><td>2</td><td>(1,1)</td><td>1/36</td></tr> <tr><td>3</td><td>(1,2), (2,1)</td><td>2/36</td></tr> <tr><td>4</td><td>(1,3), (2,2), (3,1)</td><td>3/36</td></tr> <tr><td>5</td><td>(1,4), (2,3), (3,2), (4,1)</td><td>4/36</td></tr> <tr><td>6</td><td>(1,5), (2,4), (3,3), (4,2), (5,1)</td><td>5/36</td></tr> <tr><td>7</td><td>(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)</td><td>6/36</td></tr> <tr><td>8</td><td>(2,6), (3,5), (4,4), (5,3), (6,2)</td><td>5/36</td></tr> <tr><td>9</td><td>(3,6), (4,5), (5,4), (6,3)</td><td>4/36</td></tr> <tr><td>10</td><td>(4,6), (5,5), (6,4)</td><td>3/36</td></tr> <tr><td>11</td><td>(5,6), (6,5)</td><td>2/36</td></tr> <tr><td>12</td><td>(6,6)</td><td>1/36</td></tr> </tbody>
    </table>
    <p> We can neatly visualize the <strong>probability mass function</strong> (PMF) for the sum of two dice like this:
    </p>
    <figure style="text-align: center;">
        <img src="/CLT_plots/dice2.png" alt="PMF - sum of two dice toss" style="width: {{ page.img_width }}; display: block; margin: auto;"/>
        <figcaption style="font-size: smaller; color: gray;">Probability mass function for the sum of two dice tosses.</figcaption>
    </figure>
    <p>
        Now, one might wonder: are we doomed to manually count the number of possibilities for each outcome whenever we want to bring in more dice? Well, yes and no. Ideally, we'd like a formula that gives us, for the number of dice $n$ and a possible outcome $k$, the number of possibilities $N(k; n)$, or the probability $p(k; n) = N(k; n) / (6^n)$. To do that, we use a cute little math trick.
    </p>
    <p>
    This trick uses polynomials to sort out the possible outcomes of dice tosses. First, notice that multiplying two terms, each in the form $x^{\text{some power}}$, simply causes the powers to add:
    </p>
    <p style="text-align: center;">
        $$
            x^a x^b = x^{a + b}
        $$
    </p>
    <p>
        Notice, how this immediately gives us an outcome of a very specific toss: a die with all sides marked "$a$" and another with all sides marked "$b$". What would a more realistic dice toss look like in terms of a polynomial? Since a die has sides numbered from 1 through 6, the following polynomial will represent such die
    </p>
    <p style="text-align: center;">
        $$
            P_1(x) \equiv x^1 + x^2 + x^3 + x^4 + x^5 + x^6
        $$
    </p>
    <p>
    In a sense, this polynomial hides all possible outcomes of a single die toss in its terms. How about tossing two dice? We simply multiply two copies of this polynomial:
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                P_2(x) &= \left( P_1 (x) \right)^2 = \left( x^1 + x^2 + x^3 + x^4 + x^5 + x^6 \right) = \\
                &= x^2 + 2 x^3 + 3 x^4 + 4 x^5 + 5 x^6 + 6 x^7 + 5 x^8 + 4 x^9 + 3 x^{10} + 2 x^{11} + x^{12}
            \end{aligned}
        $$
    </p>
    <p>
    The numbers 1, 2, 3, ... we see here are exactly the number of ways to achieve sums of 2, 3, 4, ... ! When we expand the polynomials and group together terms with the same power of $x$, the prefactor corresponds to the number of ways that power appears in the product. This gives us the general expression:
    </p>
    <p style="text-align: center;">
        $$
            N (k; n) = \text{coefficient of $x^k$ in } P_n (x), \quad P_n (x) \equiv \left( P_1 (x) \right)^n
        $$
    </p>
    <p>
    In other words, the frequencies $N (k; n)$ appear as the coefficients of the expansion of the function $P_n (x)$,
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = \sum_{k = n}^{6n} N (k; n) x^k
        $$
    </p>
    <p>
    In this sense, $P_n (x)$ is the <strong>generating function</strong> of our frequency coefficients of $n$ dice toss sum. To move towards a more useful formula, we now look at the expression $P_1 (x)$ again. We start by taking out a common factor of x
    </p>
    <p style="text-align: center;">
        $$
            P_1 (x) = x \left( 1 + x + x^2 + x^3 + x^4 + x^5 \right)
        $$
    </p>
    <p>
    Now we use a well known fact about polynomial series to simplify the expression $S \equiv 1 + x + \cdots + x^5$ (or other general power, since the logic is not dependent on the last coefficient being specifically $x^5$). We compare $S$ and $x S$:
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                S &= 1 + x + x^2 + x^3 + x^4 + x^5 \\
                x S &= x + x^2 + x^3 + x^4 + x^5 + x^6
            \end{aligned}
        $$
    </p>
    <p>
    We see, that the two differ only by the first term in $S$ (1) and last term in $x S$ ($x^6$). Thus, their difference is
    </p>
    <p style="text-align: center;">
        $$
            S - x S = (1 - x) S = 1 - x^6 \quad \implies \quad 1 + x + x^2 + x^3 + x^4 + x^5 = S = \frac{1 - x^6}{1 - x}
        $$
    </p>
    <p>
    Going back to the generating function $P_n (x)$, we now see
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = \left( P_1 (x) \right)^n = x^n \left( \frac{1 - x^6}{1 - x} \right)^n
        $$
    </p>
    <p>
    The reason we brought the generating function to this form is that the resulting expansion is somewhat simpler than using the multinomial theorem on the sum $1 + x + \cdots + x^5$. We now need to expand all pieces of $P_n$ and put them together. We start with the denominator, which is given by a simple geometric series
    </p>
    <p style="text-align: center;">
        $$
            \frac{1}{1 - x} = \sum_{j = 0}^\infty x^j = 1 + x + x^2 + \cdots
        $$
    </p>
    <p>
    Luckily, the $n$-th power of this series can be handled by using derivatives instead of a complicated multinomial expansion. First, we notice a pattern
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{1 - x} &= \frac{1}{(1 - x)^2} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{(1 - x)^2} &= \frac{2}{(1 - x)^3} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{(1 - x)^3} &= \frac{2 \cdot 3}{(1 - x)^4} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{(1 - x)^4} &= \frac{2 \cdot 3 \cdot 4}{(1 - x)^5}
            \end{aligned}
        $$
    </p>
    <p>
    from which we deduce the following general result (prove of which is left as an exercise to the reader)
    </p>
    <p style="text-align: center;">
        $$
            \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \frac{1}{1 - x} = \frac{(n - 1)!}{(1 - x)^n} \quad \implies \quad \frac{1}{(1 - x)^n} = \frac{1}{(n - 1)!} \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \frac{1}{1 - x}
        $$
    </p>
    <p>
    Let's unleash this on our geometric series. We know, that the first non-vanishing term will be $x^{n - 1}$
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                \frac{1}{(1 - x)^n} &= \frac{1}{(n - 1)!} \frac{\mathrm{d}^{n - 1}}{\mathrm{d} x^{n - 1}} \left( x^{n - 1} + x^n + x^{n + 1} + \cdots \right) \\
                &= \frac{1}{(n - 1)!} \left( (n - 1)! + \frac{n!}{1!} x + \frac{(n+1)!}{2!} x^2 + \cdots \right) = \\
                &= 1 + \frac{n!}{(n - 1)! \; 1!} x + \frac{(n + 1)!}{(n - 1)! \; 2!} x^2 + \cdots = \sum_{j = 0}^\infty \frac{(n + j - 1)!}{(n-1)! \; j!} x^j
            \end{aligned}
        $$
    </p>
    <p>
    Notice, that the term in the last expression is equivalent to the following binomial coefficient
    </p>
    <p style="text-align: center;">
        $$
            \frac{(n + j - 1)!}{(n-1)! \; j!} = \binom{n + j - 1}{n - 1}
        $$
    </p>
    <p>
    Now we express the numerator $(1 - x^6)^n$ in terms of a sum using binomial theorem
    </p>
    <p style="text-align: center;">
        $$
            (1 - x^6)^n = \sum_{\ell = 0}^n (-1)^\ell \binom{n}{\ell} x^{6 \ell}
        $$
    </p>
    <p>
    Finally, putting everything together,
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = x^n \sum_{j = 0}^\infty \binom{n + j - 1}{n - 1} x^j \sum_{\ell = 0}^n (-1)^\ell \binom{n}{\ell} x^{6 \ell}
        $$
    </p>
    <p>
    Let's go back to our original goal; determining the frequency $N (k ; n)$. For a given $n \leq k \leq 6n$, the coefficient of $x^k$ in $P_n (x)$ is given by a condition $n + j + 6 \ell = k$, from which we can fix $j$ and thus get rid of one of the sums
    </p>
    <p style="text-align: center;">
        $$
            N (k;n) = \sum_{\ell = 0}^{\ell_{\text{max}}} (-1)^\ell \binom{k - 6 \ell - 1}{n - 1} \binom{n}{\ell}
        $$
    </p>
    <p>
    The upper limit of this sum can be fixed by demanding that $k - 6 \ell - 1 \geq n - 1$, and thus $\ell \leq (k - n) / 6$. If $(k - n) / 6$ is not an integer, then the upper limit is given by the largest integer not exceeding $(k - n) / 6$, i.e. the floor of $(k - n) / 6$, $\ell_{\text{max}} = \lfloor (k - n) / 6 \rfloor$.
    </p>
    <p>
    We can now use our new formula for $N(k;n)$ to find the probability distribution for $n = 3, 4, ...$
    </p>
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice3.png" alt="Plot 1" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice4.png" alt="Plot 2" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice5.png" alt="Plot 3" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice6.png" alt="Plot 3" style="width: 100%;">
        </figure>
    </div>
    <p>
    However, due to the sum, the formula is barely better than manually counting the cases. We need something better. Let's now take $n$ to be large; dozens, hundreds, no, <strong>thousands</strong> of tosses. What is the distribution of the frequencies of the sums in that case? First, we introduce a modified random variable, $y$, that is corrected for the mean and standard deviation in $N (k; n)$. The mean is simple; we know, that the mean value of the outcome of $n$ dice is $((1 + 6) / 2) n = (7/2) n$. As for the variance, we recall our generating function for $N (k; n)$ and how it relates to the probability distribution
    </p>
    <p style="text-align: center;">
        $$
            P_n (x) = \sum_{k = n}^{6n} N (k; n) x^k
        $$
    </p>
    <p>
    Using $p (k; n) = N (k; n) / 6^n$ to rewrite the above expression for probability instead of frequency, we find
    </p>
    <p style="text-align: center;">
        $$
            \frac{1}{6^n} P_n (x) = \sum_{k = n}^{6n} p (k; n) x^k = \left\langle x^k \right\rangle
        $$
    </p>
    <p>
    where we realized that the sum over probabilities corresponds to a certain, special, parametric, mean value (here $x$ is the parameter, while $k$ is a random variable we sum over). That's right, the function $P_n (x) / 6^n$ corresponds to the moment-generating function of the dice toss distribution! Knowing this, we can calculate various expectation values (average quantities). For example, we already know that all probabilities should sum up to 1, but still,
    </p>
    <p style="text-align: center;">
        $$
            \sum_{k = n}^{6n} p (k; n) = 1 = \frac{1}{6^n} P_n (1) = \frac{\left( 1^1 + 1^2 + 1^3 + 1^4 + 1^5 + 1^6 \right)^n}{6^n} = \frac{6^n}{6^n} = 1
        $$
    </p>
    <p>
    What about the mean value? We need to calculate the following sum:
    </p>
    <p style="text-align: center;">
        $$
            \left\langle k \right\rangle = \sum_{k = n}^{6n} p (k; n) \, k
        $$
    </p>
    <p>
    But our original moment-generating function contains terms $x^k$, not $k$. However, we can use the following neat derivative trick:
    </p>
    <p style="text-align: center;">
        $$
            \frac{\mathrm{d}}{\mathrm{d} x} x^k = k x^{k-1} \quad \to \quad k = \left. \frac{\mathrm{d}}{\mathrm{d} x} x^k \right|_{x = 1}
        $$
    </p>
    <p>
    We can now apply this to the moment-generating function. Since the sum is finite, we can just differentiate it term by term (note - the coefficients do not contain $x$, so the derivative passes through them like Taco Bell passes through my digestive track on Friday evening) and obtain
    </p>
    <p style="text-align: center;">
        $$
            \left\langle k \right\rangle = \sum_{k = n}^{6n} p (k; n) \, \left. \frac{\mathrm{d}}{\mathrm{d} x} x^k \right|_{x = 1} = \left. \frac{\mathrm{d}}{\mathrm{d} x} \sum_{k = n}^{6n} p (k; n) \, x^k \right|_{x = 1}
        $$
    </p>
    <p>
    At this point, we can replace the sum with the moment-generating function and get
    </p>
    <p style="text-align: center;">
        $$
            \left\langle k \right\rangle = \left. \frac{\mathrm{d}}{\mathrm{d} x} \frac{1}{6^n} P_n (x) \right|_{x = 1} = \frac{1}{6^n} P_n^\prime (1)
        $$
    </p>
    <p>
    Now, what is $P_n^\prime (1)$? Glad you asked, calculating mean values was never easier:
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                P_n^\prime (1) &= \left. \frac{\mathrm{d}}{\mathrm{d} x} \left[ \left( x + x^2 + x^3 + x^4 + x^5 + x^6 \right)^n \right] \right|_{x = 1} = \\
                &= \left. n \left( x + x^2 + x^3 + x^4 + x^5 + x^6 \right)^{n - 1} \left( 1 + 2 x + 3 x + 4 x + 5 x + 6 x \right) \right|_{x = 1} = \\
                &= 21 \cdot 6^{n - 1} n
            \end{aligned}
        $$
    </p>
    <p>
    Putting back the factor $1 / 6^n$, we get
    </p>
    <p style="text-align: center;">
        $$
            \mu = \left\langle k \right\rangle = \frac{21 \cdot 6^{n - 1}}{6^n} n = \frac{21}{6} n = \frac{7}{2} n
        $$
    </p>
    <p>
    as expected. Now that we know the method works, we can calculate another mean required for variance; $\langle k^2 \rangle$. To do that, we use derivatives again. First, we express $k^2$ in terms of $x^k$ as follows
    </p>
    <p style="text-align: center;">
        $$
            \left. \begin{aligned}
                \frac{\mathrm{d}}{\mathrm{d} x} x^k &= k x^{k-1} \\
                \frac{\mathrm{d}}{\mathrm{d} x} \left( x \frac{\mathrm{d}}{\mathrm{d} x} x^k \right) &= k^2 x^{k-1}
            \end{aligned} \right\} \quad \implies \quad
            k^2 = \left. \frac{\mathrm{d}}{\mathrm{d} x} \left( x \frac{\mathrm{d}}{\mathrm{d} x} x^k \right) \right|_{x = 1}
        $$
    </p>
    <p>
    This time, the calculation gives us variance (left as a homework exercise for your favorite algebra software)
    </p>
    <p style="text-align: center;">
        $$
            \sigma^2 = \left\langle k^2 \right\rangle - \left\langle k \right\rangle^2 = \frac{35}{12} n
        $$
    </p>
    Now, let's define a normalized random variable
    </p>
    <p style="text-align: center;">
        $$
            y = \frac{k - \mu}{\sigma}
        $$
    </p>
    <p>
    Our goal is to figure out the <strong>probability density function</strong> (PDF) of $y$ in the limit of large $n$. While the variable $k$ is discrete (only having integer values), in the limit of large $n$, variable $y$ becomes "more and more continuous", to the point we can speak of a probability distribution, instead of probability mass function. To proceed with this daunting task, we first look at the characteristic function of the distribution for $k$, $\phi_k^{(n)} (t)$. As usual, it is defined as the following cryptic mean value
    </p>
    <p style="text-align: center;">
        $$
            \phi_k^{(n)} (t) = \left\langle e^{i t k} \right\rangle
        $$
    </p>
    <p>
    However, since we already know the moment generating function, we can directly relate the characteristic function to $P_n$ where we replace $x = e^{i t}$
    </p>
    <p style="text-align: center;">
        $$
            \phi_k^{(n)} (t) = \frac{1}{6^n} e^{i n t} \left( \frac{1 - e^{6 i t}}{1 - e^{i t}} \right)^n
        $$
    </p>
    <p>
    Since the new variable $y$ is $k$ shifted by $\mu$ and rescaled by $\sigma$, we can directly write down the characteristic function of $y$
    </p>
    <p style="text-align: center;">
        $$
            \phi_y^{(n)} (t) = \left\langle e^{i t y} \right\rangle = \left\langle e^{i t (k - \mu) / \sigma} \right\rangle = e^{- i \mu t / \sigma} \phi_k^{(n)} (t / \sigma)
        $$
    </p>
    <p>
    Finally, to see what happens when $n$ is large, we expand this expression in $t$ around $t = 0$:
    </p>
    <p style="text-align: center;">
        $$
            \phi_y^{(n)} (t) = 1 - \frac{t^2}{2} + \left( \frac{1}{8} - \frac{37}{700 n} \right) t^4 + \left( - \frac{1}{48} + \frac{37}{1400 n} - \frac{1333}{128625 n^2} \right) t^6 + \cdots
        $$
    </p>
    <p>
    This expression is very similar to that of a standard Gaussian, $f (y) = (2 \pi)^{-1/2} \exp \left( - \frac{1}{2} y^2 \right)$. The characteristic function of such Gaussian is
    </p>
    <p style="text-align: center;">
        $$
            \phi_y (t) = \exp \left( - \frac{1}{2} t^2 \right) = 1 - \frac{t^2}{2} + \frac{t^4}{8} - \frac{t^6}{48} + \cdots
        $$
    </p>
    <p>
    As we can see, the terms in the characteristic function for the dice toss converge to that of the Gaussian in the limit $n \to \infty$. To show this exactly would require a more careful proof. But I hope this is convincing enough argument to see why this process in particular ends up getting closer and closer to the bell-shaped curve with growing $n$. We can check the histograms for actual dice tosses for larger $n$ and see that the process converges really quickly (here I also added the corresponding Gaussian curve for comparison):
    </p>
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice10.png" alt="Plot 1" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice20.png" alt="Plot 2" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice30.png" alt="Plot 3" style="width: 100%;">
        </figure>
        <figure style="margin: 0;">
            <img src="/CLT_plots/dice40.png" alt="Plot 3" style="width: 100%;">
        </figure>
    </div>
    <div style="height: 30px;"></div>
    <p>
    So now we see why, at least in the case of a coin toss, summing up large number of random elements, results in a Gaussian distribution. Next time we look at what about this changes for continuous processes (not much).
    </p>
    <p>
    Bonus: this whole analysis can be repeated for a $s$-sided coin. If you wonder whether the convergence to a Gaussian is faster or slower, it's neither; changing the number of sides barely changes anything about the speed of convergence. The coefficients next to $t^4$ and $t^6$ can be obtained through expansion:
    </p>
    <p style="text-align: center;">
        $$
            \begin{aligned}
                &\frac{1}{8} - \frac{s^2 + 1}{s^2 - 1} \frac{1}{n} \\
                &\frac{1}{48} + \frac{s^2 + 1}{40 (s^2 - 1)} \frac{1}{n} - \frac{s^4 + s^2 + 1}{105 (s^2 - 1)^2} \frac{1}{n^2}
            \end{aligned}
        $$
    </p>
    <p>
    and as you can see, increasing $s$ has very little effect on the size of the terms containing $n$. Thus, increasing the number of sides on a dices has virtually no effect on the speed of the convergence towards a Gaussian.
    </p>
</div>
