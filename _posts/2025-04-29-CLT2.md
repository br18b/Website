---
layout: post
title: "Why Gaussian? Part II: continuous additive processes"
permalink: /blog/why-gaussian-part-ii/
date: 2025-04-29 12:00:00 -0400
categories: article
img_width: "65%"
highlight: true
---

<div style="flex: 1; text-align: justify;">
    <h3>See <a href="{{ '/blog/why-gaussian-part-i/' | relative_url }}">Why Gaussian? Part I: tossing dice</a> for part 1!</h3>
    <p>
        In the first part on this topic, <a href="{{ '/blog/why-gaussian-part-i/' | relative_url }}">Why Gaussian? Part I: tossing dice</a>, we explored how tossing more and more dice and summing the result of all tosses gives us a number that's distributed closer and closer to a Gaussian. In the second part, we will observe the same thing happening for continuous processes, where the underlying numbers come from a distribution over real numbers (or a continuous interval).
    </p>
    <p>
        We start by defining a <strong>source distribution</strong> - it can be anything not too pathological (it should have well-defined mean and variance), $\mathcal{D} (x)$. We can verify the CLT for a process, in which we don't get a number directly from the source distribution, rather, we get an average of $n$ values generated by this random process
    </p>
    <p style="text-align: center;">
            $$
                X = \frac{1}{n} \sum_{i = 1}^n x_i, \quad x_i \sim \mathcal{D}
            $$
        </p>
    <p>
        We will see, that value $X$ follows a distribution closer and closer to Gaussian if $n$ is large. We will see this by taking an <strong>ensemble</strong> of $N$ samples $(X_1, X_2, \dots, X_N)$. Since the overall mean and variance might, over many samples $n$, drift far away from the origin (especially if the source distribution does not have zero mean), we <strong>renormalize</strong> the samples by subtracting the ensemble mean and dividing by variance:
    </p>
    <p style="text-align: center;">
        $$
            Y_k = \frac{X_k - \mu_X}{\sigma_X}, \quad \mu_X = \frac{1}{N} \sum_{k = 1}^N X_k, \quad \sigma_X^2 = \frac{1}{N} \sum_{k = 1}^N \left( X_k - \mu \right)^2
        $$
    </p>
    <p>
        Now we (or rather the central limit theorem) claim that the distribution of $Y_k$ will tend to the normal distribution if $n$ is large enough. Let's verify this using Python! We start by importing all the required modules:
    </p>
    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import os</code></pre>
    <p>
        Here numpy is required to generate the random samples, matplotlib for making neat plots and os for I/O. Having done that, we now proceed with the function that will generate the samples, renormalize them, create histograms and save the histograms. Thinking ahead, we know we will need several inputs: the random distribution, $n$, $N$. Additionally, we will also control the number of bins to create the resulting distribution. We can include more arguments to make plotting more modular, but ultimately, the core of the function is captured with the first 4 arguments.
    </p>
    <pre><code class="language-python">def clt_demo(
    dist_func, # source random distribution
    n=5, # baseline number of random variables to sum
    N=10000, # size of the ensemble
    bins=50, # number of bins for visualization
    base_dir="",
    title="CLT Demo",
    filename="plot.png"
    bounds=[-10,10] # cutoff if the distribution is weird
):</code></pre>
    <p>
        The neat thing about Python is its vectorization capabilities; almost all built-in functions are capable of taking in an array of arbitrary shape which then results in output of the same shape, where the function is applied to each element. We can utilize this by generating $N \times n$ values all at once in a big $N \times n$ array, which we then sum over the last index. Specifically,
    </P>
    <pre><code class="language-python">samples = dist_func(size=(N, n))
sums = np.sum(samples, axis=1)</code></pre>
    <p>
        For really high $n$ and $N$, we would implement a different strategy where the sum is being performed on the fly to save the memory. Having done this, the rest of the code simply normalizes the ensemble, plots the histogram and saves it. Full code with some examples:
    </p>
    <button onclick="toggleCode()" style="margin-bottom: 10px;">Show Code</button>

<div id="codeBlock" style="display: none;">
  <pre><code class="language-python">import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
from scipy.stats import norm

def clt*demo(
dist_func, # function to draw random samples
n=5, # number of random variables to sum
N=10000, # how many sums to create
bins=100, # number of bins for the histogram
base_dir="",
title="CLT Demo",
filename="plot.png", # filename to save
bounds=[-4.5,4.5] # cutoff if the distribution is weird
): # create the ensemble. Tracking progress with tqdm
sums = np.zeros(N)
for * in tqdm(range(n), desc="Adding random samples"):
sums += dist_func(size=N)

    # mean and variance
    mean = np.mean(sums)
    std = np.std(sums)

    # normalize ensemble
    normalized = (sums - mean) / std

    normalized = normalized[(normalized > bounds[0]) &
     (normalized < bounds[1])]

    # plot and save
    plt.figure(figsize=(8, 5))

    # histogram
    counts, bins_edges, _ = plt.hist(
        normalized, bins=bins, density=True, alpha=0.7,
        color="skyblue", edgecolor="black", label="Ensemble histogram"
    )

    # overplot ideal normal distribution
    x = np.linspace(bounds[0], bounds[1], 1000)
    y = norm.pdf(x, loc=0, scale=1)  # zero mean, unit variance
    plt.plot(x, y, 'k--', label="Ideal Normal Distribution")

    plt.title(f"{title}\n(n={n}, N={N})")
    plt.xlabel("Normalized Sum")
    plt.ylabel("Probability Density")
    plt.grid(True)
    plt.legend()

    # save figure
    full_path = os.path.join(base_dir, filename)
    plt.savefig(full_path, dpi=150)
    plt.close()
    print(f"Saved plot to {full_path}")

# fun distributions

def uniform_distribution(size):
return np.random.uniform(low=0.0, high=1.0, size=size)

def exponential_distribution(size):
return np.random.exponential(scale=1.0, size=size)

def bernoulli_distribution(size):
return np.random.choice([0, 1], size=size)

def heavy_tail_distribution(size):
return np.random.standard_cauchy(size=size)

def sample*custom_tail(alpha, size):
u = np.random.uniform(low=0.0, high=1.0, size=size)
s = np.sign(2 * u - 1)
transformed = ( (np.pi / 2)\*\*(1/(1+alpha)) \_ np.abs(2*u - 1) )\*\*(1+alpha)
x = s * (np.tan(transformed))\*\*(1/(1+alpha))
return x

spiky1 = lambda size: sample_custom_tail(alpha=0, size=size)
spiky2 = lambda size: sample_custom_tail(alpha=0.5, size=size)
spiky3 = lambda size: sample_custom_tail(alpha=1, size=size)
spiky4 = lambda size: sample_custom_tail(alpha=1.5, size=size)
spiky5 = lambda size: sample_custom_tail(alpha=2, size=size)
spiky6 = lambda size: sample_custom_tail(alpha=3, size=size)

Npts = 10000000

dir = 'CLT_plots'
os.makedirs(dir, exist_ok=True)

# Example runs

for n in [1,2,3,4,5,10,50,100,1000]:
    clt_demo(spiky1, n=n, N=Npts, base_dir=dir, filename="Cauchy1_n"+str(n)+".png", title="Gen. Cauchy, alpha ~ 2")
    clt_demo(spiky2, n=n, N=Npts, base_dir=dir, filename="Cauchy2_n"+str(n)+".png", title="Gen. Cauchy, alpha ~ 2.5")
    clt_demo(spiky3, n=n, N=Npts, base_dir=dir, filename="Cauchy3_n"+str(n)+".png", title="Gen. Cauchy, alpha ~ 3")
    clt_demo(spiky4, n=n, N=Npts, base_dir=dir, filename="Cauchy4_n"+str(n)+".png", title="Gen. Cauchy, alpha ~ 3.5")
    clt_demo(spiky5, n=n, N=Npts, base_dir=dir, filename="Cauchy5_n"+str(n)+".png", title="Gen. Cauchy, alpha ~ 4")
    clt_demo(spiky6, n=n, N=Npts, base_dir=dir, filename="Cauchy6_n"+str(n)+".png", title="Gen. Cauchy, alpha ~ 5")
    clt_demo(uniform_distribution, n=n, N=Npts, base_dir=dir, filename="uniform_n"+str(n)+".png", title="Uniform Distribution")
    clt_demo(exponential_distribution, n=n, N=Npts, base_dir=dir, filename="exponential_n"+str(n)+".png", title="Exponential Distribution")
    clt_demo(bernoulli_distribution, n=n, N=Npts, base_dir=dir, filename="bernoulli_n"+str(n)+".png", title="Bernoulli Distribution")
</code></pre>

</div>
<script>
    function toggleCode() {
        var x = document.getElementById("codeBlock");
        var button = event.target;
        if (x.style.display === "none") {
            x.style.display = "block";
            button.innerText = "Hide Code";
        } else {
            x.style.display = "none";
            button.innerText = "Show Code";
        }
    }
    </script>
        <p>
            We can now look at some examples. We start with the uniform baseline distribution that generates numbers between 0 and 1 with equal probability. With that, the expected histogram for $n = 1$ is flat:
        </p>
        <figure style="text-align: center;">
            <img src="{{ site.baseurl }}/CLT_plots/uniform_n1.png" alt="Uniform Distribution" width="100%"/>
            <figcaption style="font-size: smaller; color: gray;">Uniform Distribution.</figcaption>
        </figure>
        <p>
            The range changed due to the normalization to zero mean and unit variance. Now, if we increase $n$ to $2, 3, 4, 5$, we get closer and closer to the Gaussian distribution:
        </p>
        <!-- Carousel 1: Uniform Distribution -->
    <div class="plot-carousel">
    <button class="carousel-btn left" onclick="changePlot('carousel1', -1)" title="Click to see previous n">&#8592;</button>
    <img id="carousel1-img" src="{{ site.baseurl }}/CLT_plots/uniform_n1.png" alt="Plot for n = 1">
    <button class="carousel-btn right" onclick="changePlot('carousel1', 1)" title="Click to see next n">&#8594;</button>
    </div>
    <p>
        Convergence towards a distribution is a very rich and interesting topic; not all distributions will lead to the Gaussian; a heavy-tailed Cauchy distribution will converge to another Cauchy distribution, for example. However, as long as the distribution is "well-behaved", convergence is assured. Berry-Esseen's theorem guarantees a relatively slow convergence (the difference between Gaussian and the distribution after summing $n$ terms is on the order of $1/\sqrt{n}$) provided the third moment $\langle x^3 \rangle$ exists and is finite. We saw that the convergence for the uniform distribution is rather quick; this is because the third moment is actually zero, in which case the convergence is slightly faster ($1/n$). The behavior for the Cauchy distribution can be observed here:
    </p>
    <!-- Carousel 2: Cauchy Distribution -->
    <div class="plot-carousel">
    <button class="carousel-btn left" onclick="changePlot('carousel2', -1)" title="Click to see previous n">&#8592;</button>
    <img id="carousel2-img" src="{{ site.baseurl }}/CLT_plots/Cauchy1_n1.png" alt="Plot for n = 1">
    <button class="carousel-btn right" onclick="changePlot('carousel2', 1)" title="Click to see next n">&#8594;</button>
    </div>
    <script>
    const carouselData = {
        carousel1: {
        index: 0,
        images: [
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n1.png", label: "n = 1" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n2.png", label: "n = 2" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n3.png", label: "n = 3" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n5.png", label: "n = 5" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n10.png", label: "n = 10" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n20.png", label: "n = 20" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n30.png", label: "n = 30" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n40.png", label: "n = 40" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n50.png", label: "n = 50" },
            { src: "{{ site.baseurl }}/CLT_plots/uniform_n100.png", label: "n = 100" }
        ]
        },
        carousel2: {
        index: 0,
        images: [
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n1.png", label: "n = 1" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n2.png", label: "n = 2" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n3.png", label: "n = 3" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n5.png", label: "n = 5" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n10.png", label: "n = 10" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n20.png", label: "n = 20" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n30.png", label: "n = 30" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n40.png", label: "n = 40" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n50.png", label: "n = 50" },
            { src: "{{ site.baseurl }}/CLT_plots/Cauchy1_n100.png", label: "n = 100" }
        ]
        }
    };
    function changePlot(carouselId, direction) {
        const data = carouselData[carouselId];
        data.index = (data.index + direction + data.images.length) % data.images.length;
        const img = document.getElementById(`${carouselId}-img`);
        img.src = data.images[data.index].src;
        img.alt = `Plot for ${data.images[data.index].label}`;
        const container = img.parentElement;
        const leftBtn = container.querySelector(".carousel-btn.left");
        const rightBtn = container.querySelector(".carousel-btn.right");
        leftBtn.title = `Click to see ${data.images[(data.index - 1 + data.images.length) % data.images.length].label}`;
        rightBtn.title = `Click to see ${data.images[(data.index + 1) % data.images.length].label}`;
    }
    </script>
    <p>
        As we can see, the Cauchy distribution never truly strays from its original shape. With its very heavy tails ($f(x) \sim x^{-2}$), the Cauchy distribution lacks a well-defined mean or variance, and therefore doesn't satisfy the conditions of the central limit theorem. Nevertheless, for a carefully chosen width, its behavior under averaging can still stabilize in a meaningful way.
    </p>
    <p>
        Stay tuned for analytic derivation of the observed convergence!
    </p>
</div>
